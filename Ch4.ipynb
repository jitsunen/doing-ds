{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import dill as dl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1000</td>\n",
       "      <td>963</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://www.nytimes.com/2018/03/05/style/vanit...</td>\n",
       "      <td>11 of Our Best Weekend Reads</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" itemId=\"https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "count                                                1000   \n",
       "unique                                               1000   \n",
       "top     https://www.nytimes.com/2018/03/05/style/vanit...   \n",
       "freq                                                    1   \n",
       "\n",
       "                               title  \\\n",
       "count                           1000   \n",
       "unique                           963   \n",
       "top     11 of Our Best Weekend Reads   \n",
       "freq                              15   \n",
       "\n",
       "                                                     body  \n",
       "count                                                1000  \n",
       "unique                                               1000  \n",
       "top     <!DOCTYPE html><html lang=\"en\" itemId=\"https:/...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = [('arts', 'Ch4/Arts.csv'), \n",
    "            ('business','Ch4/Business.csv'), \n",
    "            ('obituaries','Ch4/Obituaries.csv'), \n",
    "            ('sports','Ch4/Sports.csv'), \n",
    "            ('world', 'Ch4/World.csv')]\n",
    "for section,file in sections:\n",
    "    with open(file) as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.split(sep='\\t', maxsplit=2) for x in content] \n",
    "        globals()[section] = pd.DataFrame(content, columns=['url', 'title', 'body'])\n",
    "\n",
    "arts.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process title and body. tokenize and remove stop-words\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def get_tokenized_text(text):\n",
    "    # remove style and script elements\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # tokenize and remove stop words\n",
    "    tokens = word_tokenize(soup.get_text())\n",
    "    # remove single quotes, lower the strings\n",
    "    tokens = list(map(lambda w: w.replace(\"'\", \"\").lower(), tokens))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return ','.join(tokens)\n",
    "\n",
    "# tokenize title and body\n",
    "for t,s in sections:\n",
    "    globals()[t]['title_tokenized'] = globals()[t].apply(lambda row: get_tokenized_text(row['title']), axis = 1)\n",
    "    globals()[t]['body_tokenized'] = globals()[t].apply(lambda row: get_tokenized_text(row['body']), axis = 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_tokenized</th>\n",
       "      <th>body_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nytimes.com/2018/04/26/arts/design...</td>\n",
       "      <td>10 Galleries to Visit Now on the Upper East Side</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;!--[if (gt IE 9)|!(IE)]&gt; &lt;!--&gt;...</td>\n",
       "      <td>10,galleries,visit,upper,east,side</td>\n",
       "      <td>10,galleries,visit,upper,east,side,-,new,york,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.nytimes.com/aponline/2018/04/26/sp...</td>\n",
       "      <td>The Latest: Lions, Bengals Each Draft Centers</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;!--[if (gt IE 9)|!(IE)]&gt; &lt;!--&gt;...</td>\n",
       "      <td>latest,:,lions,,,bengals,draft,centers</td>\n",
       "      <td>latest,:,packers,cowboys,add,defensive,stars,-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2018/04/26/sports/nfl-...</td>\n",
       "      <td>NFL Draft 2018 Live: Round 1 Pick-by-Pick Updates</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;!--[if (gt IE 9)|!(IE)]&gt; &lt;!--&gt;...</td>\n",
       "      <td>nfl,draft,2018,live,:,round,1,pick-by-pick,upd...</td>\n",
       "      <td>nfl,draft,2018,live,:,round,1,pick-by-pick,upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.nytimes.com/2018/04/26/nyregion/la...</td>\n",
       "      <td>The Lawyer at the Side of de Blasio, Cuomo and...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;!--[if (gt IE 9)|!(IE)]&gt; &lt;!--&gt;...</td>\n",
       "      <td>lawyer,side,de,blasio,,,cuomo,conor,mcgregor</td>\n",
       "      <td>lawyer,side,de,blasio,,,cuomo,conor,mcgregor,-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.nytimes.com/aponline/2018/04/26/sp...</td>\n",
       "      <td>Game 6s on Tap: LeBron, Raptors, Jazz Look to ...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;!--[if (gt IE 9)|!(IE)]&gt; &lt;!--&gt;...</td>\n",
       "      <td>game,6s,tap,:,lebron,,,raptors,,,jazz,look,adv...</td>\n",
       "      <td>game,6s,tap,:,lebron,,,raptors,,,jazz,look,adv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.nytimes.com/2018/04/26/arts/design...   \n",
       "1  https://www.nytimes.com/aponline/2018/04/26/sp...   \n",
       "2  https://www.nytimes.com/2018/04/26/sports/nfl-...   \n",
       "3  https://www.nytimes.com/2018/04/26/nyregion/la...   \n",
       "4  https://www.nytimes.com/aponline/2018/04/26/sp...   \n",
       "\n",
       "                                               title  \\\n",
       "0   10 Galleries to Visit Now on the Upper East Side   \n",
       "1      The Latest: Lions, Bengals Each Draft Centers   \n",
       "2  NFL Draft 2018 Live: Round 1 Pick-by-Pick Updates   \n",
       "3  The Lawyer at the Side of de Blasio, Cuomo and...   \n",
       "4  Game 6s on Tap: LeBron, Raptors, Jazz Look to ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
       "1  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
       "2  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
       "3  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
       "4  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
       "\n",
       "                                     title_tokenized  \\\n",
       "0                 10,galleries,visit,upper,east,side   \n",
       "1             latest,:,lions,,,bengals,draft,centers   \n",
       "2  nfl,draft,2018,live,:,round,1,pick-by-pick,upd...   \n",
       "3       lawyer,side,de,blasio,,,cuomo,conor,mcgregor   \n",
       "4  game,6s,tap,:,lebron,,,raptors,,,jazz,look,adv...   \n",
       "\n",
       "                                      body_tokenized  \n",
       "0  10,galleries,visit,upper,east,side,-,new,york,...  \n",
       "1  latest,:,packers,cowboys,add,defensive,stars,-...  \n",
       "2  nfl,draft,2018,live,:,round,1,pick-by-pick,upd...  \n",
       "3  lawyer,side,de,blasio,,,cuomo,conor,mcgregor,-...  \n",
       "4  game,6s,tap,:,lebron,,,raptors,,,jazz,look,adv...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports.describe()\n",
    "sports.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   url  \\\n",
      "369  https://www.nytimes.com/2018/04/10/sports/hock...   \n",
      "573  https://www.nytimes.com/2018/03/18/world/europ...   \n",
      "\n",
      "                                                 title  \\\n",
      "369  Built to Win, the Golden Knights Did So Sooner...   \n",
      "573  A Wrestling Culture That Helps Keep Boys Away ...   \n",
      "\n",
      "                                                  body  \\\n",
      "369  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
      "573  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
      "\n",
      "                                     title_tokenized  \\\n",
      "369       built,win,,,golden,knights,sooner,expected   \n",
      "573  wrestling,culture,helps,keep,boys,away,fighting   \n",
      "\n",
      "                                        body_tokenized  \n",
      "369  built,win,,,golden,knights,sooner,expected,-,n...  \n",
      "573  wrestling,culture,helps,keep,boys,away,fightin...  \n",
      "                                                   url  \\\n",
      "467  https://www.nytimes.com/2018/04/21/pageoneplus...   \n",
      "410  https://www.nytimes.com/reuters/2018/04/23/wor...   \n",
      "\n",
      "                                                 title  \\\n",
      "467                        Corrections: April 22, 2018   \n",
      "410  Voter Support for Japan's Abe Slips Amid Calls...   \n",
      "\n",
      "                                                  body  \\\n",
      "467  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
      "410  <!DOCTYPE html><!--[if (gt IE 9)|!(IE)]> <!-->...   \n",
      "\n",
      "                                       title_tokenized  \\\n",
      "467                      corrections,:,april,22,,,2018   \n",
      "410  voter,support,japan,abe,slips,amid,calls,finan...   \n",
      "\n",
      "                                        body_tokenized  \n",
      "467  corrections,:,april,22,,,2018,-,new,york,times...  \n",
      "410  voter,support,japan,,abe,slips,amid,calls,fina...  \n"
     ]
    }
   ],
   "source": [
    "# test-train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for df,file in sections:\n",
    "    train, test = train_test_split(globals()[df], test_size=0.5)\n",
    "    globals()[df + \"_train\"] = train\n",
    "    globals()[df + \"_test\"] = test\n",
    "\n",
    "print(sports_train.head(2))\n",
    "print(business_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word vectors, across documents, for each section\n",
    "from functools import reduce\n",
    "def combine_words(w1, w2):\n",
    "    ret = list(set(  (w1.split(',') if (w1 is not None) else []) + \n",
    "                     (w2.split(',') if (w2 is not None) else []) ))\n",
    "    ret.sort()\n",
    "    return \",\".join(ret)\n",
    "\n",
    "def reduce_words(dataframe):\n",
    "    return reduce(lambda w1, w2: combine_words(w1,w2), dataframe.loc[:,'body_tokenized'])\n",
    "\n",
    "for section,file in sections:\n",
    "    globals()[section + \"_words_train\"] = reduce_words(globals()[section+\"_train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(business_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35222"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up word tokens\n",
    "#  remove any tokens less than 3 chars in size\n",
    "#  remove quote char from word beginnings\n",
    "tokens_vars = ['arts_words', 'business_words', 'obituaries_words', 'sports_words', 'world_words']\n",
    "for tokens in tokens_vars:\n",
    "    globals()[tokens + \"_train_fil\"] = list(filter(lambda word: len(word) > 3, globals()[tokens + \"_train\"].split(\",\")))\n",
    "    \n",
    "len(business_words_train_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49169\n",
      "35222\n",
      "33489\n",
      "45287\n",
      "37053\n"
     ]
    }
   ],
   "source": [
    "for s,f in sections:\n",
    "    print(len(globals()[s + \"_words_train_fil\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200220"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse all section training words into a single vector\n",
    "words_train = reduce(lambda v1,v2: v1 + v2, list(map(lambda s: globals()[s[0] + \"_words_train_fil\"], sections)))\n",
    "\n",
    "len(words_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:36:54.324593  calculating priors for arts\n",
      "20:58:50.816766  calculating priors for business\n",
      "21:17:29.807737  calculating priors for obituaries\n"
     ]
    }
   ],
   "source": [
    "# calculate probabilities from training sets\n",
    "def calc_word_priors(alpha, beta, word_tokens, dataframe):\n",
    "    word_priors = []\n",
    "    num_documents = dataframe.size\n",
    "    for word in word_tokens:\n",
    "        count = (dataframe['title_tokenized'].str.contains(\",\" + word + \",\", regex = False)).sum()\n",
    "        count = count + (dataframe['body_tokenized'].str.contains(\",\" + word + \",\", regex = False)).sum()\n",
    "        prob = (count + alpha)/(num_documents + beta)\n",
    "        word_priors.append(prob)\n",
    "    return word_priors\n",
    "\n",
    "\n",
    "import datetime\n",
    "# find priors for all training words\n",
    "words_priors = [[]] \n",
    "for section, file in sections:\n",
    "    print(datetime.datetime.now().time(), ' calculating priors for ' + section)\n",
    "    words_priors.append(calc_word_priors(1, 2, words_train, globals()[section + \"_train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_log_odds(class_prior, base_class_prior):\n",
    "    log_odds = math.log( (class_prior * (1 - base_class_prior)) / ( base_class_prior * (1 - class_prior) ) ) if (class_prior > 0.0) & (base_class_prior > 0.0) else 0\n",
    "    return log_odds\n",
    "\n",
    "def calc_base_class_weights(class_prior, base_class_prior):\n",
    "    return math.log( (1 - class_prior) / (1 - base_class_prior) ) if (class_prior > 0.0) & (base_class_prior > 0.0) else 0\n",
    "    \n",
    "# calculate weights\n",
    "words_theta = words_priors\n",
    "\n",
    "base_class_theta = words_theta[1]\n",
    "\n",
    "words_weights = [[calc_log_odds(theta, base_class_theta[index]) for index,theta in enumerate(class_theta)] for class_theta in words_theta[2:6]]\n",
    "\n",
    "words_weights_base_class = [[calc_base_class_weights(theta, base_class_theta[index]) for index,theta in enumerate(class_theta)] for class_theta in words_theta[2:6]]\n",
    "words_weights_base_class = list(map(sum, words_weights_base_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-abb8b9e6f51f>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-abb8b9e6f51f>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    aaa        prob.ppend(sum(clss_prob))\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "def calc_class_prob(document_words, words_train, words_weights, words_weights_base_class):\n",
    "    prob = []\n",
    "    for doc_class in range(5):\n",
    "        class_prob = []\n",
    "        for word in words_train:\n",
    "            word_index = document_words.index(word)\n",
    "            if( word_index != -1):\n",
    "                class_prob.append(words_weights[doc_class][word_index])\n",
    "            else:\n",
    "                class_prob.append(1 - words_weights[doc_class][word_index])\n",
    "aaa        prob.ppend(sum(cl))\n",
    "arts_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8507\n"
     ]
    }
   ],
   "source": [
    "print(len(words_weights[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-30014.99153768913, -29751.01455324569, -31099.8956275988, -28846.16115197452, -29871.516564088706]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "def calc_class_prob(document_words, words_train, words_theta):\n",
    "    prob = []\n",
    "    for doc_class in range(5):\n",
    "        class_prob = []\n",
    "        for word in words_train:\n",
    "            try:\n",
    "                word_index = words_train.index(word)\n",
    "                if( word not in document_words):\n",
    "                    class_prob.append(math.log(words_theta[doc_class + 1][word_index]))\n",
    "                else:\n",
    "                    class_prob.append(math.log(1 - words_theta[doc_class + 1][word_index]))\n",
    "            except IndexError:\n",
    "                print(doc_class, word_index)\n",
    "        prob.append(sum(class_prob))\n",
    "    return prob\n",
    "\n",
    "print(calc_class_prob(arts_words_train, words_train, words_theta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
